{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类方法有:\n",
    "\n",
    "- TF-IDF\n",
    "- Count Features\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- Xgboost\n",
    "- Grid Search\n",
    "- Word Vectors\n",
    "- Dense Network\n",
    "- LSTM\n",
    "- GRU\n",
    "- Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:15:13.918084Z",
     "start_time": "2020-03-02T03:14:33.362539Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:15:27.109847Z",
     "start_time": "2020-03-02T03:15:22.187326Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('./datasets/复旦大学中文文本分类语料.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:15:28.005850Z",
     "start_time": "2020-03-02T03:15:27.932845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>分类</th>\n",
       "      <th>正文</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>农业</td>\n",
       "      <td>﻿【 文献号 】1-784\\n【原文出处】农业技术经济\\n【原刊地名】京\\n【原刊期号】19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6267</th>\n",
       "      <td>体育</td>\n",
       "      <td>﻿【 文献号 】1-3107\\n【原文出处】启蒙\\n【原刊地名】津\\n【原刊期号】19950...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>经济</td>\n",
       "      <td>﻿【 文献号 】2-799\\n【原文出处】预测\\n【原刊地名】合肥\\n【原刊期号】20000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>艺术</td>\n",
       "      <td>﻿【 文献号 】2-568\\n【原文出处】太原日报\\n【原刊期号】19950418\\n【原刊...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>文学</td>\n",
       "      <td>﻿【 日期 】19960906\\n【 版号 】11\\n【 标题 】剖析当代知识分子心灵\\n【...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      分类                                                 正文\n",
       "4871  农业  ﻿【 文献号 】1-784\\n【原文出处】农业技术经济\\n【原刊地名】京\\n【原刊期号】19...\n",
       "6267  体育  ﻿【 文献号 】1-3107\\n【原文出处】启蒙\\n【原刊地名】津\\n【原刊期号】19950...\n",
       "9082  经济  ﻿【 文献号 】2-799\\n【原文出处】预测\\n【原刊地名】合肥\\n【原刊期号】20000...\n",
       "110   艺术  ﻿【 文献号 】2-568\\n【原文出处】太原日报\\n【原刊期号】19950418\\n【原刊...\n",
       "756   文学  ﻿【 日期 】19960906\\n【 版号 】11\\n【 标题 】剖析当代知识分子心灵\\n【..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:15:32.029927Z",
     "start_time": "2020-03-02T03:15:32.020912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9249 entries, 0 to 9248\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   分类      9249 non-null   object\n",
      " 1   正文      9249 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 144.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:15:34.875124Z",
     "start_time": "2020-03-02T03:15:34.834138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['艺术', '文学', '哲学', '通信', '能源', '历史', '矿藏', '空间', '教育', '交通', '计算机',\n",
       "       '环境', '电子', '农业', '体育', '时政', '医疗', '经济', '法律'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.分类.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以先过滤特殊符号，只保留汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本实验运行时，没有使用\n",
    "# import re\n",
    "\n",
    "\n",
    "# def preprocessing(txt, pattern=r'[\\u4e00-\\u9fa5]+'):\n",
    "#     re_tokens = re.findall(pattern, txt)\n",
    "#     return re_tokens\n",
    "\n",
    "\n",
    "# data['正则结果'] = data['正文'].apply(lambda txt: ' '.join(preprocessing(txt)))\n",
    "# # 注意修改下面分词结果，在正则结果上apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可采用分词效果更好的分词器，如pyltp、THULAC、Hanlp等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:18:50.651888Z",
     "start_time": "2020-03-02T03:18:50.647904Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ltp模型目录的路径\n",
    "LTP_DATA_DIR = r'D:\\ProgramData\\nlp_package\\ltp_v34'\n",
    "# 分词模型路径，模型名称为`cws.model`\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:28:38.815080Z",
     "start_time": "2020-03-02T03:22:12.298966Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyltp import Segmentor\n",
    "\n",
    "\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "data['分词结果'] = data['正文'].apply(lambda i: ' '.join(segmentor.segment(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:28:44.828392Z",
     "start_time": "2020-03-02T03:28:44.803394Z"
    }
   },
   "outputs": [],
   "source": [
    "segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:28:46.862393Z",
     "start_time": "2020-03-02T03:28:46.848393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>分类</th>\n",
       "      <th>正文</th>\n",
       "      <th>分词结果</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>计算机</td>\n",
       "      <td>﻿微型机与应用\\nMICROCOMPUTER &amp; ITS APPLICATIONS\\n199...</td>\n",
       "      <td>﻿ 微型机 与 应用 \\n MICROCOMPUTER &amp; ITS APPLICATIONS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7885</th>\n",
       "      <td>经济</td>\n",
       "      <td>﻿【 文献号 】2-1862\\n【原文出处】上海社会科学院学术季刊\\n【原刊期号】20000...</td>\n",
       "      <td>﻿ 【 文献号 】 2-1862 \\n 【 原文 出处 】 上海 社会 科学院 学术季 刊\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>经济</td>\n",
       "      <td>﻿【 文献号 】2-1117\\n【原文出处】财政研究\\n【原刊地名】京\\n【原刊期号】199...</td>\n",
       "      <td>﻿ 【 文献号 】 2-1117 \\n 【 原文 出处 】 财政 研究 \\n 【 原刊 地名...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>农业</td>\n",
       "      <td>﻿湖北农业科学\\nHUBEI AGRICULTURAL SCIENCES\\n1998年第6期...</td>\n",
       "      <td>﻿ 湖北 农业 科学\\n HUBEI AGRICULTURAL SCIENCES \\n 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>计算机</td>\n",
       "      <td>﻿自动化学报\\nACTA AUTOMATICA SINICA\\n2000　Vol.26　No...</td>\n",
       "      <td>﻿ 自动化学 报\\n ACTA AUTOMATICA SINICA \\n 2000 Vol....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       分类                                                 正文  \\\n",
       "3122  计算机  ﻿微型机与应用\\nMICROCOMPUTER & ITS APPLICATIONS\\n199...   \n",
       "7885   经济  ﻿【 文献号 】2-1862\\n【原文出处】上海社会科学院学术季刊\\n【原刊期号】20000...   \n",
       "8731   经济  ﻿【 文献号 】2-1117\\n【原文出处】财政研究\\n【原刊地名】京\\n【原刊期号】199...   \n",
       "5498   农业  ﻿湖北农业科学\\nHUBEI AGRICULTURAL SCIENCES\\n1998年第6期...   \n",
       "2369  计算机  ﻿自动化学报\\nACTA AUTOMATICA SINICA\\n2000　Vol.26　No...   \n",
       "\n",
       "                                                   分词结果  \n",
       "3122  ﻿ 微型机 与 应用 \\n MICROCOMPUTER & ITS APPLICATIONS...  \n",
       "7885  ﻿ 【 文献号 】 2-1862 \\n 【 原文 出处 】 上海 社会 科学院 学术季 刊\\...  \n",
       "8731  ﻿ 【 文献号 】 2-1117 \\n 【 原文 出处 】 财政 研究 \\n 【 原刊 地名...  \n",
       "5498  ﻿ 湖北 农业 科学\\n HUBEI AGRICULTURAL SCIENCES \\n 19...  \n",
       "2369  ﻿ 自动化学 报\\n ACTA AUTOMATICA SINICA \\n 2000 Vol....  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:31:13.166482Z",
     "start_time": "2020-03-02T03:31:13.159449Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"对数损失度量（Logarithmic Loss  Metric）的多分类版本。\n",
    "    :param actual: 包含actual target classes的数组\n",
    "    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "    \n",
    "    # clip 0 and 1 for calculate\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:33:07.980734Z",
     "start_time": "2020-03-02T03:33:07.975731Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(data.分类.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:34:52.364362Z",
     "start_time": "2020-03-02T03:34:52.259851Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(data.分词结果.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:35:10.332687Z",
     "start_time": "2020-03-02T03:35:10.328686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8324,)\n",
      "(925,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document Frequency)+逻辑斯底回归（Logistic Regression）\n",
    "\n",
    "将文本中的数字特征统一表示成\"#NUMBER\"，达到一定的降噪效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:41:45.607963Z",
     "start_time": "2020-03-02T03:41:45.602965Z"
    }
   },
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\" 将所有数字标记映射为一个占位符（Placeholder）。\n",
    "    对于许多实际应用场景来说，以数字开头的tokens不是很有用，\n",
    "    全部视为一类‘数字’。 通过将所有数字都表示成同一个符号，可以达到降维的目的。\n",
    "    \"\"\"\n",
    "    return ('#NUMBER' if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super(NumberNormalizingVectorizer, self).build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T03:49:59.169561Z",
     "start_time": "2020-03-02T03:49:59.132560Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('datasets\\stopwords.txt', 'r', encoding='utf-8') as f: \n",
    "    stopwords_list = [w.strip() for w in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T08:52:16.699356Z",
     "start_time": "2020-03-02T08:50:40.828400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#NUMBER', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'lex', 'll', 'mon', 'null', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = NumberNormalizingVectorizer(min_df=3,  \n",
    "                                  max_df=0.5,\n",
    "                                  max_features=None,                 \n",
    "                                  ngram_range=(1, 2), \n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  stop_words = stopwords_list)\n",
    "\n",
    "tfidf_vectorizer.fit(data.分词结果.values)\n",
    "xtrain_tfidf = tfidf_vectorizer.transform(xtrain)\n",
    "xvalid_tfidf = tfidf_vectorizer.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:05:14.256143Z",
     "start_time": "2020-03-02T08:59:47.531326Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0,solver='lbfgs',multi_class='multinomial')\n",
    "\n",
    "clf.fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:05:56.641376Z",
     "start_time": "2020-03-02T09:05:56.488375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.589 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict_proba(xvalid_tfidf)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:07:31.961002Z",
     "start_time": "2020-03-02T09:06:07.600946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'lex', 'll', 'mon', 'null', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=3,\n",
    "                                    max_df=5,\n",
    "                                    ngram_range=(1, 2),\n",
    "                                    stop_words=stopwords_list)\n",
    "\n",
    "count_vectorizer.fit(data.分词结果.values)\n",
    "xtrain_bow = count_vectorizer.transform(xtrain)\n",
    "xvalid_bow = count_vectorizer.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:09:45.889158Z",
     "start_time": "2020-03-02T09:07:42.746812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.774 \n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0,solver='lbfgs',multi_class='multinomial')\n",
    "\n",
    "clf.fit(xtrain_bow, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_bow)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:10:51.767178Z",
     "start_time": "2020-03-02T09:10:50.402209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.886 \n"
     ]
    }
   ],
   "source": [
    "# tf-idf feature\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfidf, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfidf)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:11:15.414907Z",
     "start_time": "2020-03-02T09:11:14.798908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.914 \n"
     ]
    }
   ],
   "source": [
    "# bow feature\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_bow, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_bow)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:18:05.676663Z",
     "start_time": "2020-03-02T09:16:55.893502Z"
    }
   },
   "outputs": [],
   "source": [
    "# 在使用SVM之前，我们需要将数据标准化（Standardize Data ），同时结合SVD降维\n",
    "# 对于SVM来说，SVD的components的合适调整区间一般为120~200 \n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "\n",
    "svd.fit(xtrain_tfidf)\n",
    "xtrain_svd = svd.transform(xtrain_tfidf)\n",
    "xvalid_svd = svd.transform(xvalid_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:18:31.724340Z",
     "start_time": "2020-03-02T09:18:31.681309Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scaler.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scaler.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:21:59.944117Z",
     "start_time": "2020-03-02T09:21:15.311763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.358 \n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=1.0, probability=True)\n",
    "\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:55:45.733918Z",
     "start_time": "2020-03-02T09:55:45.651888Z"
    }
   },
   "source": [
    "这部分运行很慢，lightGBM更快一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T11:50:37.756027Z",
     "start_time": "2020-03-02T11:04:46.251891Z"
    }
   },
   "outputs": [],
   "source": [
    "# 不使用sklearn API的用法\n",
    "\n",
    "# # tf-idf\n",
    "# xtrain_xgb = xgb.DMatrix(xtrain_tfidf, label=ytrain)\n",
    "# xvalid_xgb = xgb.DMatrix(xvalid_tfidf, label=yvalid)\n",
    "\n",
    "# # setup parameters for xgboost\n",
    "# param = {}\n",
    "# # use softmax multi-class classification\n",
    "# param['objective'] = 'multi:softmax'\n",
    "# # scale weight of positive examples\n",
    "# param['eta'] = 0.1\n",
    "# param['max_depth'] = 7\n",
    "# param['silent'] = 1\n",
    "# param['nthread'] = 4\n",
    "# param['num_class'] = len(data.分类.unique())\n",
    "# param['eval_metric'] = 'mlogloss'\n",
    "# param['colsample_bytree']  = 0.8\n",
    "# param['subsample']  = 0.8\n",
    "\n",
    "# num_round = 100\n",
    "\n",
    "# clf = xgb.train(param, xtrain_xgb, num_boost_round=num_round)\n",
    "\n",
    "# predictions = clf.predict(xvalid_xgb)  # class id list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:44:31.704166Z",
     "start_time": "2020-03-02T09:25:50.476278Z"
    }
   },
   "outputs": [],
   "source": [
    "# sklearn inferface\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                       max_depth=7,\n",
    "                       n_estimators=50, \n",
    "                       colsample_bytree=0.8, \n",
    "                       subsample=0.8, \n",
    "                       nthread=10, \n",
    "                       learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_tfidf.tocsc(), ytrain)  # Sparse col\n",
    "predictions = clf.predict_proba(xvalid_tfidf.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bow feature\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', \n",
    "                       max_depth=7,\n",
    "                       n_estimators=50, \n",
    "                       colsample_bytree=0.8, \n",
    "                       subsample=0.8, \n",
    "                       nthread=10, \n",
    "                       learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_bow.tocsc(), ytrain)  # Sparse col\n",
    "predictions = clf.predict_proba(xvalid_bow.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf + svd\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                       max_depth=7,\n",
    "                       n_estimators=50, \n",
    "                       colsample_bytree=0.8, \n",
    "                       subsample=0.8, \n",
    "                       nthread=10, \n",
    "                       learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_svd.tocsc(), ytrain)  # Sparse col\n",
    "predictions = clf.predict_proba(xvalid_svd.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf + svd + scaled\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                       max_depth=7,\n",
    "                       n_estimators=50, \n",
    "                       colsample_bytree=0.8, \n",
    "                       subsample=0.8, \n",
    "                       nthread=10, \n",
    "                       learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_svd_scl.tocsc(), ytrain)  # Sparse col\n",
    "predictions = clf.predict_proba(xvalid_svd_scl.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T12:19:28.813592Z",
     "start_time": "2020-03-02T12:19:28.637523Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T12:34:25.346886Z",
     "start_time": "2020-03-02T12:30:16.884601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=7,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=50, n_jobs=-1, num_leaves=31,\n",
       "               objective='multiclass', random_state=None, reg_alpha=0.0,\n",
       "               reg_lambda=0.0, silent=True, subsample=0.8,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For origin package use\n",
    "# xtrain_lgb = lgb.Dataset(xtrain_tfidf, ytrain)\n",
    "# xvalid_lgb = lgb.Dataset(xvalid_tfidf, yvalid, reference=xtrain_lgb)\n",
    "\n",
    "clf = lgb.LGBMClassifier(num_leaves=31,\n",
    "                        max_depth=7,\n",
    "                        n_estimators=50,\n",
    "                        objective='multiclass',\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T12:38:43.126335Z",
     "start_time": "2020-03-02T12:38:42.871368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.238 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict_proba(xvalid_tfidf)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipline Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T12:12:48.238206Z",
     "start_time": "2020-03-02T12:10:24.744173Z"
    }
   },
   "outputs": [],
   "source": [
    "# 运行耗时较长\n",
    "\n",
    "# 自定义评分函数\n",
    "mll_scorer = metrics.make_scorer(multiclass_logloss, \n",
    "                                    greater_is_better=False, \n",
    "                                    needs_proba=True)\n",
    "\n",
    "#SVD初始化\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Standard Scaler初始化\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# 再一次使用Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# 创建pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                        ('scl', scl),\n",
    "                        ('lr', lr_model)])\n",
    "\n",
    "# param for search\n",
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}\n",
    "\n",
    "# 网格搜索模型（Grid Search Model）初始化\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=2, n_jobs=1, iid=True, refit=True, cv=2)\n",
    "\n",
    "#fit网格搜索模型\n",
    "model.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T12:42:11.319725Z",
     "start_time": "2020-03-02T12:42:04.642456Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一行一个单独文本\n",
    "doc_word_list = [dwords.split() for dwords in data['分词结果']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:09:00.914053Z",
     "start_time": "2020-03-02T13:06:51.266001Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "# sentences = LineSentence(file_path)\n",
    "model = Word2Vec(doc_word_list, min_count=5, window=7, size=100, workers=4)\n",
    "\n",
    "# model.save('word2vec_model_100v.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:09:27.504644Z",
     "start_time": "2020-03-02T13:09:27.427663Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:12:39.471245Z",
     "start_time": "2020-03-02T13:12:39.461245Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    #该函数会将语句转化为一个标准化的向量（Normalized Vector）\n",
    "    from pyltp import Segmentor\n",
    "\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(cws_model_path)  # 加载模型\n",
    "    words = segmentor.segment(s)\n",
    "    segmentor.release()\n",
    "    words = [w for w in words if not w in stopwords_list]\n",
    "    \n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            #M.append(embeddings_index[w])\n",
    "            M.append(model.wv.get_vector(w))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T14:25:43.625105Z",
     "start_time": "2020-03-02T13:19:04.285222Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85656e7dbb324de8b58e614b2d2c5e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8324.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed93413de67d40868879d2a38a14c906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=925.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# 耗时较长，一小时左右\n",
    "xtrain_w2v = [sent2vec(x) for x in tqdm_notebook(xtrain)]\n",
    "xvalid_w2v = [sent2vec(x) for x in tqdm_notebook(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T14:30:26.925519Z",
     "start_time": "2020-03-02T14:30:26.914521Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain_w2v = np.array(xtrain_w2v)\n",
    "xvalid_w2v = np.array(xvalid_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T15:01:13.578399Z",
     "start_time": "2020-03-02T15:00:26.949774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "\n",
    "max_len = 70\n",
    "\n",
    "# 对标签进行binarize处理\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "\n",
    "# 使用 keras tokenizer\n",
    "token = text.Tokenizer(num_words=None)\n",
    "token.fit_on_texts(data.分词结果.values)\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# 对文本序列进行zero填充\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T15:04:57.347951Z",
     "start_time": "2020-03-02T15:04:56.787536Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d50efd25784b8c8c9b220d9cf9e27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422526.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#基于已有的数据集中的词汇创建一个词嵌入矩阵（Embedding Matrix）\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T15:31:08.815381Z",
     "start_time": "2020-03-02T15:31:02.077278Z"
    }
   },
   "outputs": [],
   "source": [
    "# 基于前面训练的Word2vec词向量，构建1个2层的GRU模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1024, activation='selu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(256, activation='selu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(19))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T15:46:52.830565Z",
     "start_time": "2020-03-02T15:36:40.523124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8324 samples, validate on 925 samples\n",
      "Epoch 1/50\n",
      "8324/8324 [==============================] - 23s 3ms/step - loss: 2.1174 - val_loss: 1.4871\n",
      "Epoch 2/50\n",
      "8324/8324 [==============================] - 23s 3ms/step - loss: 1.8982 - val_loss: 1.4619\n",
      "Epoch 3/50\n",
      "8324/8324 [==============================] - 27s 3ms/step - loss: 1.7642 - val_loss: 1.3354\n",
      "Epoch 4/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 1.6670 - val_loss: 1.3404\n",
      "Epoch 5/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 1.5970 - val_loss: 1.2648\n",
      "Epoch 6/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 1.5412 - val_loss: 1.2279\n",
      "Epoch 7/50\n",
      "8324/8324 [==============================] - 24s 3ms/step - loss: 1.4114 - val_loss: 1.1444\n",
      "Epoch 8/50\n",
      "8324/8324 [==============================] - 22s 3ms/step - loss: 1.3204 - val_loss: 1.1163\n",
      "Epoch 9/50\n",
      "8324/8324 [==============================] - 22s 3ms/step - loss: 1.3503 - val_loss: 1.0202\n",
      "Epoch 10/50\n",
      "8324/8324 [==============================] - 22s 3ms/step - loss: 1.3168 - val_loss: 1.1422\n",
      "Epoch 11/50\n",
      "8324/8324 [==============================] - 22s 3ms/step - loss: 1.2973 - val_loss: 1.2299\n",
      "Epoch 12/50\n",
      "8324/8324 [==============================] - 23s 3ms/step - loss: 1.2918 - val_loss: 0.9962\n",
      "Epoch 13/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 1.2745 - val_loss: 0.9543\n",
      "Epoch 14/50\n",
      "8324/8324 [==============================] - 24s 3ms/step - loss: 1.1478 - val_loss: 0.8935\n",
      "Epoch 15/50\n",
      "8324/8324 [==============================] - 25s 3ms/step - loss: 1.0831 - val_loss: 0.8410\n",
      "Epoch 16/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 1.0527 - val_loss: 0.7832\n",
      "Epoch 17/50\n",
      "8324/8324 [==============================] - 24s 3ms/step - loss: 1.0057 - val_loss: 0.7692\n",
      "Epoch 18/50\n",
      "8324/8324 [==============================] - 25s 3ms/step - loss: 0.9735 - val_loss: 0.7525\n",
      "Epoch 19/50\n",
      "8324/8324 [==============================] - 23s 3ms/step - loss: 0.9436 - val_loss: 0.7603\n",
      "Epoch 20/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 0.9140 - val_loss: 0.7143\n",
      "Epoch 21/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 0.8797 - val_loss: 0.7326\n",
      "Epoch 22/50\n",
      "8324/8324 [==============================] - 27s 3ms/step - loss: 0.8609 - val_loss: 0.7102\n",
      "Epoch 23/50\n",
      "8324/8324 [==============================] - 24s 3ms/step - loss: 0.8565 - val_loss: 0.8040\n",
      "Epoch 24/50\n",
      "8324/8324 [==============================] - 26s 3ms/step - loss: 0.9015 - val_loss: 0.7256\n",
      "Epoch 25/50\n",
      "8324/8324 [==============================] - 25s 3ms/step - loss: 0.8913 - val_loss: 0.7164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19d0e19a748>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在模型拟合时，使用early stopping这个回调函数（Callback Function）\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=50, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用Stacking的方式，对于多个效果相当的基础分类器的输出结果，在输入xgboost进行分类计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T14:31:02.007298Z",
     "start_time": "2020-03-02T14:31:02.001270Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T14:31:21.665552Z",
     "start_time": "2020-03-02T14:31:21.606805Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: 模型字典 \n",
    "        :param num_folds: cv所用的fold数量\n",
    "        :param task_type: 分类（classification） 还是回归（regression）\n",
    "        :param optimize: 优化函数，比如 AUC, logloss, F1等，必须有2个函数，即y_test 和 y_pred\n",
    "        :param lower_is_better: 优化函数（Optimization Function）的值越低越好还是越高越好\n",
    "        :param save_path: 模型保存路径\n",
    "        \"\"\"\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: 二维表格形式的训练数据\n",
    "        :param y: 二进制的, 多分类或回归\n",
    "        :return: 用于预测的模型链（Chain of Models）\n",
    "        \"\"\"\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        # 每层模型的输出shape\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:  # 第一层基础分类器输入\n",
    "                temp_train = self.training_data\n",
    "            else: # 第二层基于基础分类器结果的再分类器输入\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:  # 第二层基于基础分类器结果的再分类\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:  # 第一层基础分类\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    # valid results\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                            (model_num * self.num_classes):((model_num + 1) * self.num_classes)] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    \n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                \n",
    "                # 各个基础分类器的性能不要相差太大，否则模型效果不易提升\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "            logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): \n",
    "                        ((model_num + 1) * self.num_classes)] = temp_test_predictions\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T14:53:26.426614Z",
     "start_time": "2020-03-02T14:35:09.382762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:35:09] INFO Found 19 classes\n",
      "[22:35:09] INFO Training Level 0 Fold # 1. Model # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:39:22] INFO Predicting Level 0. Fold # 1. Model # 0\n",
      "[22:39:23] INFO Level 0. Fold # 1. Model # 0. Validation Score = 0.363176\n",
      "[22:39:23] INFO Training Level 0 Fold # 2. Model # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:40] INFO Predicting Level 0. Fold # 2. Model # 0\n",
      "[22:43:40] INFO Level 0. Fold # 2. Model # 0. Validation Score = 0.368816\n",
      "[22:43:40] INFO Training Level 0 Fold # 3. Model # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\envs\\nlplearning\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:48:07] INFO Predicting Level 0. Fold # 3. Model # 0\n",
      "[22:48:07] INFO Level 0. Fold # 3. Model # 0. Validation Score = 0.339124\n",
      "[22:48:07] INFO Level 0. Model # 0. Mean Score = 0.357038. Std Dev = 0.012875\n",
      "[22:48:07] INFO Training Level 0 Fold # 1. Model # 1\n",
      "[22:49:49] INFO Predicting Level 0. Fold # 1. Model # 1\n",
      "[22:49:50] INFO Level 0. Fold # 1. Model # 1. Validation Score = 0.974753\n",
      "[22:49:50] INFO Training Level 0 Fold # 2. Model # 1\n",
      "[22:51:09] INFO Predicting Level 0. Fold # 2. Model # 1\n",
      "[22:51:09] INFO Level 0. Fold # 2. Model # 1. Validation Score = 0.966990\n",
      "[22:51:09] INFO Training Level 0 Fold # 3. Model # 1\n",
      "[22:53:00] INFO Predicting Level 0. Fold # 3. Model # 1\n",
      "[22:53:00] INFO Level 0. Fold # 3. Model # 1. Validation Score = 0.960663\n",
      "[22:53:00] INFO Level 0. Model # 1. Mean Score = 0.967469. Std Dev = 0.005762\n",
      "[22:53:00] INFO Training Level 0 Fold # 1. Model # 2\n",
      "[22:53:01] INFO Predicting Level 0. Fold # 1. Model # 2\n",
      "[22:53:01] INFO Level 0. Fold # 1. Model # 2. Validation Score = 0.934614\n",
      "[22:53:01] INFO Training Level 0 Fold # 2. Model # 2\n",
      "[22:53:02] INFO Predicting Level 0. Fold # 2. Model # 2\n",
      "[22:53:02] INFO Level 0. Fold # 2. Model # 2. Validation Score = 0.923922\n",
      "[22:53:02] INFO Training Level 0 Fold # 3. Model # 2\n",
      "[22:53:03] INFO Predicting Level 0. Fold # 3. Model # 2\n",
      "[22:53:04] INFO Level 0. Fold # 3. Model # 2. Validation Score = 0.830623\n",
      "[22:53:04] INFO Level 0. Model # 2. Mean Score = 0.896386. Std Dev = 0.046706\n",
      "[22:53:04] INFO Training Level 0 Fold # 1. Model # 3\n",
      "[22:53:04] INFO Predicting Level 0. Fold # 1. Model # 3\n",
      "[22:53:04] INFO Level 0. Fold # 1. Model # 3. Validation Score = 2.621586\n",
      "[22:53:04] INFO Training Level 0 Fold # 2. Model # 3\n",
      "[22:53:05] INFO Predicting Level 0. Fold # 2. Model # 3\n",
      "[22:53:05] INFO Level 0. Fold # 2. Model # 3. Validation Score = 2.362605\n",
      "[22:53:05] INFO Training Level 0 Fold # 3. Model # 3\n",
      "[22:53:05] INFO Predicting Level 0. Fold # 3. Model # 3\n",
      "[22:53:05] INFO Level 0. Fold # 3. Model # 3. Validation Score = 2.189225\n",
      "[22:53:05] INFO Level 0. Model # 3. Mean Score = 2.391139. Std Dev = 0.177660\n",
      "[22:53:05] INFO Saving predictions for level # 0\n",
      "[22:53:07] INFO Training Level 1 Fold # 1. Model # 0\n",
      "[22:53:13] INFO Predicting Level 1. Fold # 1. Model # 0\n",
      "[22:53:13] INFO Level 1. Fold # 1. Model # 0. Validation Score = 0.383380\n",
      "[22:53:13] INFO Training Level 1 Fold # 2. Model # 0\n",
      "[22:53:19] INFO Predicting Level 1. Fold # 2. Model # 0\n",
      "[22:53:19] INFO Level 1. Fold # 2. Model # 0. Validation Score = 0.384011\n",
      "[22:53:19] INFO Training Level 1 Fold # 3. Model # 0\n",
      "[22:53:25] INFO Predicting Level 1. Fold # 3. Model # 0\n",
      "[22:53:25] INFO Level 1. Fold # 3. Model # 0. Validation Score = 0.369997\n",
      "[22:53:25] INFO Level 1. Model # 0. Mean Score = 0.379129. Std Dev = 0.006462\n",
      "[22:53:25] INFO Saving predictions for level # 1\n"
     ]
    }
   ],
   "source": [
    "#为每个level的集成指定使用数据：\n",
    "train_data_dict = {0: [xtrain_tfidf, xtrain_bow, xtrain_tfidf, xtrain_bow], 1: [xtrain_w2v]}\n",
    "test_data_dict = {0: [xvalid_tfidf, xvalid_bow, xvalid_tfidf, xvalid_bow], 1: [xvalid_w2v]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(),\n",
    "                      LogisticRegression(), \n",
    "                      MultinomialNB(alpha=0.1), \n",
    "                      MultinomialNB()],\n",
    "              1: [xgb.XGBClassifier(silent=True, \n",
    "                                    objective='multi:softmax',\n",
    "                                    n_estimators=25, \n",
    "                                    max_depth=6,\n",
    "                                    colsample_bytree=0.8, \n",
    "                                    subsample=0.8, \n",
    "                                    learning_rate=0.1)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
    "\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_w2v.shape[0])\n",
    "\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_w2v.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T14:55:59.680645Z",
     "start_time": "2020-03-02T14:55:59.660646Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4194193846150403"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 损失\n",
    "multiclass_logloss(yvalid, preds[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "228.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
