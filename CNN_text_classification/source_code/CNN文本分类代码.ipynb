{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将文本整合到 train、test、val 三个文件中\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "def _read_file(filename):\n",
    "    \"\"\"读取一个文件并转换为一行\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        # \\u3000 是全角的空白符，\\xa0 是不间断空白符\n",
    "        return f.read().replace('\\n', '').replace('\\t', '').replace('\\u3000', '')\n",
    "\n",
    "def save_file(dirname):\n",
    "    \"\"\"\n",
    "    将多个文件整合并存到3个文件中\n",
    "    dirname: 原数据目录\n",
    "    文件内容格式:  类别\\t内容\n",
    "    \"\"\"\n",
    "    f_train = open('data/cnews/cnews.train.txt', 'w', encoding='utf-8')\n",
    "    f_test = open('data/cnews/cnews.test.txt', 'w', encoding='utf-8')\n",
    "    f_val = open('data/cnews/cnews.val.txt', 'w', encoding='utf-8')\n",
    "    for category in os.listdir(dirname):   # 分类文本目录\n",
    "        cat_dir = os.path.join(dirname, category)\n",
    "        if not os.path.isdir(cat_dir):\n",
    "            continue\n",
    "        files = os.listdir(cat_dir)\n",
    "        count = 0\n",
    "        for cur_file in files:\n",
    "            filename = os.path.join(cat_dir, cur_file)\n",
    "            content = _read_file(filename)\n",
    "            if count < 5000:\n",
    "                f_train.write(category + '\\t' + content + '\\n')\n",
    "            elif count < 6000:\n",
    "                f_test.write(category + '\\t' + content + '\\n')\n",
    "            else:\n",
    "                f_val.write(category + '\\t' + content + '\\n')\n",
    "            count += 1\n",
    "\n",
    "        print('Finished:', category)\n",
    "\n",
    "    f_train.close()\n",
    "    f_test.close()\n",
    "    f_val.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     save_file('data/thucnews')\n",
    "#     print(len(open('data/cnews/cnews.train.txt', 'r', encoding='utf-8').readlines()))\n",
    "#     print(len(open('data/cnews/cnews.test.txt', 'r', encoding='utf-8').readlines()))\n",
    "#     print(len(open('data/cnews/cnews.val.txt', 'r', encoding='utf-8').readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cnews_loader文件操作\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "\n",
    "if sys.version_info[0] > 2:\n",
    "    is_py3 = True\n",
    "else:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False\n",
    "\n",
    "\n",
    "def native_word(word, encoding='utf-8'):\n",
    "    \"\"\"如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\"\"\"\n",
    "    if not is_py3:\n",
    "        return word.encode(encoding)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "\n",
    "def open_file(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    常用文件操作，可在python2和python3间切换.\n",
    "    mode: 'r' or 'w' for read or write\n",
    "    \"\"\"\n",
    "    if is_py3:\n",
    "        return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        return open(filename, mode)\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(native_content(content)))\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')\n",
    "\n",
    "\n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [native_content(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "\n",
    "    categories = [native_content(x) for x in categories]\n",
    "\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "    return categories, cat_to_id\n",
    "\n",
    "\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(\n",
    "        label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad\n",
    "\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cnn model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "\n",
    "    embedding_dim = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度\n",
    "    num_classes = 10  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表达小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"文本分类，CNN模型\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # 三个待输入的数据\n",
    "        self.input_x = tf.placeholder(\n",
    "            tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(\n",
    "            tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.cnn()\n",
    "\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN模型\"\"\"\n",
    "        # 词向量映射\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable(\n",
    "                'embedding',\n",
    "                [self.config.vocab_size, self.config.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(\n",
    "                embedding_inputs,\n",
    "                self.config.num_filters,\n",
    "                self.config.kernel_size,\n",
    "                name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 全连接层，后面接dropout以及relu激活\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # 分类器\n",
    "            self.logits = tf.layers.dense(\n",
    "                fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # 损失函数，交叉熵\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # 优化器\n",
    "            self.optim = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 准确率\n",
    "            correct_pred = tf.equal(\n",
    "                tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练，测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "主程序文件\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "from cnn_model import TCNNConfig, TextCNN\n",
    "from data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n",
    "\n",
    "base_dir = 'data/cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "WARNING:tensorflow:From D:\\jupyter_code\\NLP\\NLP_天善智能\\7-CNN\\文本分类\\cnn_model.py:66: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:17\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    2.3, Train Acc:   9.38%, Val Loss:    2.3, Val Acc:   9.76%, Time: 0:00:05 *\n",
      "Iter:    100, Train Loss:   0.77, Train Acc:  76.56%, Val Loss:    1.1, Val Acc:  70.78%, Time: 0:00:10 *\n",
      "Iter:    200, Train Loss:   0.43, Train Acc:  87.50%, Val Loss:   0.61, Val Acc:  82.54%, Time: 0:00:15 *\n",
      "Iter:    300, Train Loss:   0.31, Train Acc:  92.19%, Val Loss:   0.45, Val Acc:  86.24%, Time: 0:00:20 *\n",
      "Iter:    400, Train Loss:   0.21, Train Acc:  92.19%, Val Loss:   0.37, Val Acc:  89.06%, Time: 0:00:24 *\n",
      "Iter:    500, Train Loss:   0.24, Train Acc:  96.88%, Val Loss:   0.31, Val Acc:  91.30%, Time: 0:00:29 *\n",
      "Iter:    600, Train Loss:  0.079, Train Acc:  98.44%, Val Loss:   0.29, Val Acc:  91.78%, Time: 0:00:34 *\n",
      "Iter:    700, Train Loss:   0.19, Train Acc:  93.75%, Val Loss:   0.29, Val Acc:  91.82%, Time: 0:00:39 *\n",
      "Epoch: 2\n",
      "Iter:    800, Train Loss:   0.11, Train Acc:  93.75%, Val Loss:   0.26, Val Acc:  92.94%, Time: 0:00:44 *\n",
      "Iter:    900, Train Loss:   0.19, Train Acc:  96.88%, Val Loss:   0.26, Val Acc:  93.34%, Time: 0:00:49 *\n",
      "Iter:   1000, Train Loss:   0.14, Train Acc:  92.19%, Val Loss:   0.22, Val Acc:  93.42%, Time: 0:00:54 *\n",
      "Iter:   1100, Train Loss:  0.098, Train Acc:  96.88%, Val Loss:   0.23, Val Acc:  93.10%, Time: 0:01:00 \n",
      "Iter:   1200, Train Loss:   0.16, Train Acc:  98.44%, Val Loss:   0.22, Val Acc:  93.80%, Time: 0:01:04 *\n",
      "Iter:   1300, Train Loss:  0.099, Train Acc:  98.44%, Val Loss:   0.21, Val Acc:  93.46%, Time: 0:01:09 \n",
      "Iter:   1400, Train Loss:  0.092, Train Acc:  96.88%, Val Loss:   0.24, Val Acc:  93.84%, Time: 0:01:13 *\n",
      "Iter:   1500, Train Loss:  0.054, Train Acc:  98.44%, Val Loss:   0.23, Val Acc:  92.08%, Time: 0:01:18 \n",
      "Epoch: 3\n",
      "Iter:   1600, Train Loss:   0.11, Train Acc:  96.88%, Val Loss:    0.2, Val Acc:  94.04%, Time: 0:01:23 *\n",
      "Iter:   1700, Train Loss:  0.064, Train Acc:  98.44%, Val Loss:   0.21, Val Acc:  92.92%, Time: 0:01:28 \n",
      "Iter:   1800, Train Loss:  0.048, Train Acc:  98.44%, Val Loss:    0.2, Val Acc:  93.58%, Time: 0:01:33 \n",
      "Iter:   1900, Train Loss:   0.13, Train Acc:  95.31%, Val Loss:   0.21, Val Acc:  93.16%, Time: 0:01:38 \n",
      "Iter:   2000, Train Loss:  0.091, Train Acc:  98.44%, Val Loss:   0.19, Val Acc:  94.04%, Time: 0:01:43 \n",
      "Iter:   2100, Train Loss:  0.033, Train Acc: 100.00%, Val Loss:    0.2, Val Acc:  93.60%, Time: 0:01:48 \n",
      "Iter:   2200, Train Loss:   0.04, Train Acc:  98.44%, Val Loss:   0.21, Val Acc:  93.30%, Time: 0:01:53 \n",
      "Iter:   2300, Train Loss: 0.0083, Train Acc: 100.00%, Val Loss:    0.2, Val Acc:  93.68%, Time: 0:01:57 \n",
      "Epoch: 4\n",
      "Iter:   2400, Train Loss:  0.026, Train Acc:  98.44%, Val Loss:   0.21, Val Acc:  94.14%, Time: 0:02:02 *\n",
      "Iter:   2500, Train Loss:  0.038, Train Acc:  96.88%, Val Loss:   0.17, Val Acc:  95.24%, Time: 0:02:07 *\n",
      "Iter:   2600, Train Loss:  0.027, Train Acc:  98.44%, Val Loss:   0.21, Val Acc:  93.70%, Time: 0:02:12 \n",
      "Iter:   2700, Train Loss:   0.14, Train Acc:  96.88%, Val Loss:   0.21, Val Acc:  93.54%, Time: 0:02:16 \n",
      "Iter:   2800, Train Loss:  0.034, Train Acc:  98.44%, Val Loss:   0.19, Val Acc:  94.80%, Time: 0:02:21 \n",
      "Iter:   2900, Train Loss:   0.03, Train Acc: 100.00%, Val Loss:   0.19, Val Acc:  94.86%, Time: 0:02:25 \n",
      "Iter:   3000, Train Loss:  0.024, Train Acc: 100.00%, Val Loss:   0.19, Val Acc:  94.86%, Time: 0:02:29 \n",
      "Iter:   3100, Train Loss:  0.025, Train Acc: 100.00%, Val Loss:   0.25, Val Acc:  92.80%, Time: 0:02:34 \n",
      "Epoch: 5\n",
      "Iter:   3200, Train Loss: 0.0031, Train Acc: 100.00%, Val Loss:    0.2, Val Acc:  95.04%, Time: 0:02:38 \n",
      "Iter:   3300, Train Loss:  0.017, Train Acc: 100.00%, Val Loss:   0.17, Val Acc:  95.44%, Time: 0:02:43 *\n",
      "Iter:   3400, Train Loss:  0.034, Train Acc:  98.44%, Val Loss:    0.2, Val Acc:  94.48%, Time: 0:02:48 \n",
      "Iter:   3500, Train Loss:  0.012, Train Acc: 100.00%, Val Loss:   0.18, Val Acc:  95.92%, Time: 0:02:53 *\n",
      "Iter:   3600, Train Loss:  0.005, Train Acc: 100.00%, Val Loss:   0.21, Val Acc:  94.30%, Time: 0:02:58 \n",
      "Iter:   3700, Train Loss:  0.014, Train Acc: 100.00%, Val Loss:    0.2, Val Acc:  94.98%, Time: 0:03:02 \n",
      "Iter:   3800, Train Loss:  0.019, Train Acc: 100.00%, Val Loss:   0.22, Val Acc:  93.90%, Time: 0:03:06 \n",
      "Iter:   3900, Train Loss:  0.015, Train Acc:  98.44%, Val Loss:   0.22, Val Acc:  94.54%, Time: 0:03:11 \n",
      "Epoch: 6\n",
      "Iter:   4000, Train Loss: 0.0012, Train Acc: 100.00%, Val Loss:   0.18, Val Acc:  95.62%, Time: 0:03:15 \n",
      "Iter:   4100, Train Loss:  0.012, Train Acc: 100.00%, Val Loss:   0.23, Val Acc:  94.32%, Time: 0:03:20 \n",
      "Iter:   4200, Train Loss: 0.0055, Train Acc: 100.00%, Val Loss:   0.24, Val Acc:  94.40%, Time: 0:03:24 \n",
      "Iter:   4300, Train Loss: 0.0092, Train Acc: 100.00%, Val Loss:    0.3, Val Acc:  92.60%, Time: 0:03:29 \n",
      "Iter:   4400, Train Loss: 0.0026, Train Acc: 100.00%, Val Loss:   0.24, Val Acc:  93.16%, Time: 0:03:33 \n",
      "Iter:   4500, Train Loss: 0.0034, Train Acc: 100.00%, Val Loss:   0.22, Val Acc:  94.54%, Time: 0:03:38 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id,\n",
    "                                    config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id,\n",
    "                                config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "            #print(\"x_batch is {}\".format(x_batch.shape))\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc],\n",
    "                                                    feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(\n",
    "                    msg.format(total_batch, loss_train, acc_train, loss_val,\n",
    "                               acc_val, time_dif, improved_str))\n",
    "\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id,\n",
    "                                  config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(\n",
    "            model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(\n",
    "        metrics.classification_report(\n",
    "            y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    config = TCNNConfig()\n",
    "    if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "        build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "    categories, cat_to_id = read_category()\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    config.vocab_size = len(words)\n",
    "    model = TextCNN(config)\n",
    "    option = 'train'\n",
    "    if option == 'train':\n",
    "        train()\n",
    "    else:\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/textcnn\\best_validation\n",
      "Testing...\n",
      "Test Loss:   0.14, Test Acc:  96.49%\n",
      "Precision, Recall and F1-Score...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         体育       1.00      1.00      1.00      1000\n",
      "         财经       0.97      0.98      0.98      1000\n",
      "         房产       1.00      0.99      1.00      1000\n",
      "         家居       0.99      0.88      0.93      1000\n",
      "         教育       0.87      0.96      0.91      1000\n",
      "         科技       0.95      0.98      0.97      1000\n",
      "         时尚       0.97      0.97      0.97      1000\n",
      "         时政       0.95      0.93      0.94      1000\n",
      "         游戏       0.99      0.97      0.98      1000\n",
      "         娱乐       0.97      0.98      0.97      1000\n",
      "\n",
      "avg / total       0.97      0.96      0.96     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[998   0   0   0   2   0   0   0   0   0]\n",
      " [  0 984   0   0   5   3   0   8   0   0]\n",
      " [  0   0 995   1   3   0   0   0   0   1]\n",
      " [  1  13   0 878  48  15  15  24   0   6]\n",
      " [  0   4   0   4 957  12   6  10   3   4]\n",
      " [  0   1   0   2   7 981   3   2   4   0]\n",
      " [  2   0   0   2  10   2 973   0   1  10]\n",
      " [  0  14   0   1  39   9   0 933   1   3]\n",
      " [  0   1   0   0  13   4   4   0 974   4]\n",
      " [  1   1   0   3  10   5   4   0   0 976]]\n",
      "Time usage: 0:00:07\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predict文件\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as kr\n",
    "\n",
    "from cnn_model import TCNNConfig, TextCNN\n",
    "from data.cnews_loader import read_category, read_vocab\n",
    "\n",
    "try:\n",
    "    bool(type(unicode))\n",
    "except NameError:\n",
    "    unicode = str\n",
    "\n",
    "base_dir = 'data/cnews'\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "class CnnModel:\n",
    "    def __init__(self):\n",
    "        self.config = TCNNConfig()\n",
    "        self.categories, self.cat_to_id = read_category()\n",
    "        self.words, self.word_to_id = read_vocab(vocab_dir)\n",
    "        self.config.vocab_size = len(self.words)\n",
    "        self.model = TextCNN(self.config)\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess=self.session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    def predict(self, message):\n",
    "        # 支持不论在python2还是python3下训练的模型都可以在2或者3的环境下运行\n",
    "        content = unicode(message)\n",
    "        data = [self.word_to_id[x] for x in content if x in self.word_to_id]\n",
    "\n",
    "        feed_dict = {\n",
    "            self.model.input_x:\n",
    "            kr.preprocessing.sequence.pad_sequences([data],\n",
    "                                                    self.config.seq_length),\n",
    "            self.model.keep_prob:\n",
    "            1.0\n",
    "        }\n",
    "\n",
    "        y_pred_cls = self.session.run(\n",
    "            self.model.y_pred_cls, feed_dict=feed_dict)\n",
    "        return self.categories[y_pred_cls[0]]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cnn_model = CnnModel()\n",
    "    test_demo = [\n",
    "        '三星ST550以全新的拍摄方式超越了以往任何一款数码相机',\n",
    "        '热火vs骑士前瞻：皇帝回乡二番战 东部次席唾手可得新浪体育讯北京时间3月30日7:00'\n",
    "    ]\n",
    "    for i in test_demo:\n",
    "        print(cnn_model.predict(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
