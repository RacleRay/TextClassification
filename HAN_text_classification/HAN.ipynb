{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T15:52:09.663672Z",
     "start_time": "2019-08-21T15:31:28.515606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring HAN model...\n",
      "WARNING:tensorflow:From D:\\jupyter_code\\Deep_Learning\\Project\\NLP_Project\\Text_Classification\\github\\HAN_text_classification\\HANModel.py:190: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From D:\\jupyter_code\\Deep_Learning\\Project\\NLP_Project\\Text_Classification\\github\\HAN_text_classification\\HANModel.py:191: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From D:\\jupyter_code\\Deep_Learning\\Project\\NLP_Project\\Text_Classification\\github\\HAN_text_classification\\HANModel.py:99: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:15\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    2.3, Train Acc:  12.50%, Val Loss:    2.3, Val Acc:   8.18%, Time: 0:00:06 *\n",
      "Iter:    100, Train Loss:    2.1, Train Acc:  15.62%, Val Loss:    2.4, Val Acc:   8.34%, Time: 0:00:38 *\n",
      "Iter:    200, Train Loss:    1.7, Train Acc:  28.12%, Val Loss:    1.9, Val Acc:  23.68%, Time: 0:01:10 *\n",
      "Iter:    300, Train Loss:    1.1, Train Acc:  57.03%, Val Loss:    1.3, Val Acc:  49.42%, Time: 0:01:40 *\n",
      "Epoch: 2\n",
      "Iter:    400, Train Loss:    1.1, Train Acc:  53.12%, Val Loss:    1.2, Val Acc:  56.20%, Time: 0:02:11 *\n",
      "Iter:    500, Train Loss:    1.0, Train Acc:  64.84%, Val Loss:    1.1, Val Acc:  57.68%, Time: 0:02:41 *\n",
      "Iter:    600, Train Loss:   0.87, Train Acc:  73.44%, Val Loss:    1.1, Val Acc:  59.30%, Time: 0:03:12 *\n",
      "Iter:    700, Train Loss:   0.83, Train Acc:  69.53%, Val Loss:    1.0, Val Acc:  60.54%, Time: 0:03:43 *\n",
      "Epoch: 3\n",
      "Iter:    800, Train Loss:   0.75, Train Acc:  74.22%, Val Loss:    1.0, Val Acc:  61.72%, Time: 0:04:15 *\n",
      "Iter:    900, Train Loss:   0.75, Train Acc:  75.78%, Val Loss:   0.98, Val Acc:  62.54%, Time: 0:04:44 *\n",
      "Iter:   1000, Train Loss:   0.67, Train Acc:  74.22%, Val Loss:   0.96, Val Acc:  63.60%, Time: 0:05:15 *\n",
      "Iter:   1100, Train Loss:   0.77, Train Acc:  75.00%, Val Loss:   0.92, Val Acc:  65.18%, Time: 0:05:47 *\n",
      "Epoch: 4\n",
      "Iter:   1200, Train Loss:   0.52, Train Acc:  81.25%, Val Loss:   0.92, Val Acc:  65.22%, Time: 0:06:20 *\n",
      "Iter:   1300, Train Loss:   0.61, Train Acc:  83.59%, Val Loss:   0.88, Val Acc:  66.88%, Time: 0:06:51 *\n",
      "Iter:   1400, Train Loss:   0.57, Train Acc:  81.25%, Val Loss:   0.86, Val Acc:  68.34%, Time: 0:07:23 *\n",
      "Iter:   1500, Train Loss:   0.54, Train Acc:  82.81%, Val Loss:   0.85, Val Acc:  67.96%, Time: 0:07:53 \n",
      "Epoch: 5\n",
      "Iter:   1600, Train Loss:   0.53, Train Acc:  83.59%, Val Loss:   0.81, Val Acc:  71.28%, Time: 0:08:24 *\n",
      "Iter:   1700, Train Loss:   0.55, Train Acc:  84.38%, Val Loss:   0.78, Val Acc:  72.64%, Time: 0:08:55 *\n",
      "Iter:   1800, Train Loss:    0.5, Train Acc:  84.38%, Val Loss:   0.76, Val Acc:  73.70%, Time: 0:09:27 *\n",
      "Iter:   1900, Train Loss:   0.44, Train Acc:  85.94%, Val Loss:    0.7, Val Acc:  76.34%, Time: 0:09:57 *\n",
      "Epoch: 6\n",
      "Iter:   2000, Train Loss:   0.42, Train Acc:  88.28%, Val Loss:   0.61, Val Acc:  81.00%, Time: 0:10:27 *\n",
      "Iter:   2100, Train Loss:   0.25, Train Acc:  92.97%, Val Loss:   0.61, Val Acc:  80.50%, Time: 0:10:57 \n",
      "Iter:   2200, Train Loss:   0.42, Train Acc:  89.84%, Val Loss:   0.58, Val Acc:  81.96%, Time: 0:11:28 *\n",
      "Iter:   2300, Train Loss:   0.51, Train Acc:  83.59%, Val Loss:   0.58, Val Acc:  82.12%, Time: 0:12:00 *\n",
      "Epoch: 7\n",
      "Iter:   2400, Train Loss:   0.24, Train Acc:  92.97%, Val Loss:   0.54, Val Acc:  83.72%, Time: 0:12:31 *\n",
      "Iter:   2500, Train Loss:   0.32, Train Acc:  90.62%, Val Loss:   0.55, Val Acc:  83.54%, Time: 0:13:01 \n",
      "Iter:   2600, Train Loss:   0.25, Train Acc:  94.53%, Val Loss:   0.54, Val Acc:  83.62%, Time: 0:13:32 \n",
      "Iter:   2700, Train Loss:   0.34, Train Acc:  90.62%, Val Loss:   0.52, Val Acc:  84.52%, Time: 0:14:02 *\n",
      "Epoch: 8\n",
      "Iter:   2800, Train Loss:   0.32, Train Acc:  92.19%, Val Loss:   0.52, Val Acc:  84.82%, Time: 0:14:32 *\n",
      "Iter:   2900, Train Loss:   0.26, Train Acc:  92.19%, Val Loss:   0.51, Val Acc:  84.96%, Time: 0:15:03 *\n",
      "Iter:   3000, Train Loss:   0.32, Train Acc:  89.84%, Val Loss:   0.52, Val Acc:  85.34%, Time: 0:15:33 *\n",
      "Iter:   3100, Train Loss:   0.34, Train Acc:  90.62%, Val Loss:    0.5, Val Acc:  86.18%, Time: 0:16:04 *\n",
      "Epoch: 9\n",
      "Iter:   3200, Train Loss:   0.19, Train Acc:  95.31%, Val Loss:    0.5, Val Acc:  85.78%, Time: 0:16:36 \n",
      "Iter:   3300, Train Loss:   0.18, Train Acc:  95.31%, Val Loss:   0.49, Val Acc:  86.34%, Time: 0:17:08 *\n",
      "Iter:   3400, Train Loss:   0.39, Train Acc:  89.84%, Val Loss:    0.5, Val Acc:  86.32%, Time: 0:17:38 \n",
      "Iter:   3500, Train Loss:   0.29, Train Acc:  92.19%, Val Loss:   0.49, Val Acc:  86.50%, Time: 0:18:09 *\n",
      "Epoch: 10\n",
      "Iter:   3600, Train Loss:   0.15, Train Acc:  96.88%, Val Loss:   0.47, Val Acc:  87.18%, Time: 0:18:40 *\n",
      "Iter:   3700, Train Loss:   0.26, Train Acc:  92.97%, Val Loss:   0.47, Val Acc:  87.44%, Time: 0:19:14 *\n",
      "Iter:   3800, Train Loss:   0.33, Train Acc:  89.06%, Val Loss:   0.47, Val Acc:  87.60%, Time: 0:19:45 *\n",
      "Iter:   3900, Train Loss:    0.3, Train Acc:  89.06%, Val Loss:   0.47, Val Acc:  87.82%, Time: 0:20:16 *\n"
     ]
    }
   ],
   "source": [
    "# %load run_han.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "from HANModel import HANConfig, HANModel\n",
    "from data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n",
    "\n",
    "base_dir = 'data/cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/HANModel'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def feed_data(x_batch, word_nums, sentence_nums, labels, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.inputs: x_batch,\n",
    "        model.word_lengths: word_nums,\n",
    "        model.sentence_lengths: sentence_nums,\n",
    "        model.labels: labels,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        x_batch = np.reshape(x_batch, (batch_len, int(config.seq_length/30), 30))\n",
    "        word_lengths = np.zeros((batch_len, int(config.seq_length/30))) + 30\n",
    "        sentence_lengths = np.zeros((batch_len,)) + int(config.seq_length/30)\n",
    "\n",
    "        feed_dict = feed_data(x_batch,\n",
    "                              word_lengths,\n",
    "                              sentence_lengths,\n",
    "                              y_batch,\n",
    "                              1.0)\n",
    "        y_pred_class,loss, acc = sess.run([model.y_pred_cls,model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return y_pred_class,total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/HANModel'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            # 取每个句子长为30\n",
    "            batch_len = len(x_batch)\n",
    "            x_batch = np.reshape(x_batch, (batch_len, int(config.seq_length/30), 30))\n",
    "            word_lengths = np.zeros((batch_len, int(config.seq_length/30))) + 30\n",
    "            sentence_lengths = np.zeros((batch_len,)) + int(config.seq_length/30)\n",
    "\n",
    "            feed_dict = feed_data(x_batch,\n",
    "                                  word_lengths,\n",
    "                                  sentence_lengths,\n",
    "                                  y_batch,\n",
    "                                  config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                _, loss_val, acc_val = evaluate(session, x_val, y_val)\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    y_pred, loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        batch_len = len(x_test[start_id:end_id])\n",
    "        x_batch = np.reshape(x_test[start_id:end_id], (batch_len, int(config.seq_length/30), 30))\n",
    "        word_lengths = np.zeros((batch_len, int(config.seq_length/30))) + 30\n",
    "        sentence_lengths = np.zeros((batch_len,)) + int(config.seq_length/30)\n",
    "\n",
    "        feed_dict = {\n",
    "            model.inputs: x_batch,\n",
    "            model.word_lengths: word_lengths,\n",
    "            model.sentence_lengths: sentence_lengths,\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    print('Configuring HAN model...')\n",
    "    config = HANConfig()\n",
    "    if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "        build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "    categories, cat_to_id = read_category()\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    config.vocab_size = len(words)\n",
    "    model = HANModel(config)\n",
    "\n",
    "    option='train'\n",
    "    if option == 'train':\n",
    "        train()\n",
    "    else:\n",
    "        test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T16:00:32.513542Z",
     "start_time": "2019-08-21T16:00:11.461015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/HANModel\\best_validation\n",
      "Testing...\n",
      "Test Loss:   0.36, Test Acc:  88.89%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          体育       0.98      0.98      0.98      1000\n",
      "          财经       0.92      0.98      0.95      1000\n",
      "          房产       0.98      0.98      0.98      1000\n",
      "          家居       0.88      0.51      0.64      1000\n",
      "          教育       0.89      0.88      0.88      1000\n",
      "          科技       0.77      0.89      0.83      1000\n",
      "          时尚       0.90      0.92      0.91      1000\n",
      "          时政       0.74      0.89      0.81      1000\n",
      "          游戏       0.95      0.91      0.93      1000\n",
      "          娱乐       0.94      0.95      0.94      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.88     10000\n",
      "weighted avg       0.89      0.89      0.88     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[978   0   0   0   3   0   1   1   0  17]\n",
      " [  0 981   3   0   5   0   0  11   0   0]\n",
      " [  0  12 983   0   0   1   0   4   0   0]\n",
      " [  2   8   2 505  22 138  41 268   9   5]\n",
      " [  6  31   1   7 875  40   3  17   6  14]\n",
      " [  0   0   0  20  29 893  35   2  21   0]\n",
      " [  0   0   1  24   1  36 917   1   9  11]\n",
      " [  2  37  13  11  31  10   0 894   1   1]\n",
      " [  1   2   0   0  13  43  15   0 913  13]\n",
      " [ 14   1   0   5   9   3  10   2   6 950]]\n",
      "Time usage: 0:00:21\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
